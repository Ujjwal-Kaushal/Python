{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15deb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\" Good morning everyone,\n",
    "Today, I am honored to speak about one of India’s greatest sons — Dr. A.P.J. Abdul Kalam.\n",
    "Known as the \"Missile Man of India\", he was a brilliant scientist and an inspiring leader.\n",
    "Born in a humble family in Rameswaram, he rose through hard work and determination.\n",
    "He played a key role in India’s space and missile programs, including the Agni and Prithvi missiles.\n",
    "Dr. Kalam also contributed to India’s Pokhran nuclear tests in 1998.\n",
    "In 2002, he became the 11th President of India, earning love and respect across all ages.\n",
    "He was a people’s president, always humble, approachable, and full of hope.\n",
    "Dr. Kalam strongly believed in the power of education, dreams, and youth.\n",
    "His famous quote, \"Dream, dream, dream. Dreams transform into thoughts and thoughts result in action,\" still inspires millions.\n",
    "He wrote many books, including the popular Wings of Fire.\n",
    "Even after his presidency, he spent his life teaching and motivating students.\n",
    "He passed away while doing what he loved — teaching — on July 27, 2015.\n",
    "Dr. Kalam's life teaches us that with hard work and integrity, anything is possible.\n",
    "Let us all strive to follow his ideals and become responsible citizens of India.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a93c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce60efad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Good morning everyone,\\nToday, I am honored to speak about one of India’s greatest sons — Dr. A.P.J.',\n",
       " 'Abdul Kalam.',\n",
       " 'Known as the \"Missile Man of India\", he was a brilliant scientist and an inspiring leader.',\n",
       " 'Born in a humble family in Rameswaram, he rose through hard work and determination.',\n",
       " 'He played a key role in India’s space and missile programs, including the Agni and Prithvi missiles.',\n",
       " 'Dr. Kalam also contributed to India’s Pokhran nuclear tests in 1998.',\n",
       " 'In 2002, he became the 11th President of India, earning love and respect across all ages.',\n",
       " 'He was a people’s president, always humble, approachable, and full of hope.',\n",
       " 'Dr. Kalam strongly believed in the power of education, dreams, and youth.',\n",
       " 'His famous quote, \"Dream, dream, dream.',\n",
       " 'Dreams transform into thoughts and thoughts result in action,\" still inspires millions.',\n",
       " 'He wrote many books, including the popular Wings of Fire.',\n",
       " 'Even after his presidency, he spent his life teaching and motivating students.',\n",
       " 'He passed away while doing what he loved — teaching — on July 27, 2015.',\n",
       " \"Dr. Kalam's life teaches us that with hard work and integrity, anything is possible.\",\n",
       " 'Let us all strive to follow his ideals and become responsible citizens of India.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8366a101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ujjwa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87072865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ujjwa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "274c0f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current NLTK data paths before modification:\n",
      "C:\\Users\\ujjwa/nltk_data\n",
      "c:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\nltk_data\n",
      "c:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\share\\nltk_data\n",
      "c:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\lib\\nltk_data\n",
      "C:\\Users\\ujjwa\\AppData\\Roaming\\nltk_data\n",
      "C:\\nltk_data\n",
      "D:\\nltk_data\n",
      "E:\\nltk_data\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "print(\"Current NLTK data paths before modification:\")\n",
    "for path in nltk.data.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2d2e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data paths after explicit addition:\n",
      "C:\\Users\\ujjwa/nltk_data\n",
      "c:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\nltk_data\n",
      "c:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\share\\nltk_data\n",
      "c:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\lib\\nltk_data\n",
      "C:\\Users\\ujjwa\\AppData\\Roaming\\nltk_data\n",
      "C:\\nltk_data\n",
      "D:\\nltk_data\n",
      "E:\\nltk_data\n",
      "C:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\nltk_data\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Define the exact path where your 'nltk_data' folder is located.\n",
    "# Based on your directory structure, it should be here:\n",
    "custom_nltk_data_path = r'C:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\nltk_data'\n",
    "\n",
    "# Add this path to NLTK's search paths, if it's not already there\n",
    "if custom_nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(custom_nltk_data_path)\n",
    "\n",
    "print(\"NLTK data paths after explicit addition:\")\n",
    "for path in nltk.data.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7878987f",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ujjwa/nltk_data'\n    - 'c:\\\\Users\\\\ujjwa\\\\OneDrive\\\\Desktop\\\\Python\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\ujjwa\\\\OneDrive\\\\Desktop\\\\Python\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ujjwa\\\\OneDrive\\\\Desktop\\\\Python\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ujjwa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\ujjwa\\\\OneDrive\\\\Desktop\\\\Python\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m words = nltk.word_tokenize(sentences[i])\n\u001b[32m      3\u001b[39m words = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m pos_tag = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(pos_tag)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m    275\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.weights = json.load(fin)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ujjwa/nltk_data'\n    - 'c:\\\\Users\\\\ujjwa\\\\OneDrive\\\\Desktop\\\\Python\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\ujjwa\\\\OneDrive\\\\Desktop\\\\Python\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ujjwa\\\\OneDrive\\\\Desktop\\\\Python\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ujjwa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\ujjwa\\\\OneDrive\\\\Desktop\\\\Python\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51b4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ad22625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data paths after explicit addition (if not present):\n",
      "C:\\Users\\ujjwa/nltk_data\n",
      "c:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\nltk_data\n",
      "c:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\share\\nltk_data\n",
      "c:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\venv\\lib\\nltk_data\n",
      "C:\\Users\\ujjwa\\AppData\\Roaming\\nltk_data\n",
      "C:\\nltk_data\n",
      "D:\\nltk_data\n",
      "E:\\nltk_data\n",
      "C:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\nltk_data\n",
      "\n",
      "NLTK data is ready.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# (The custom_nltk_data_path addition from Step 2 goes here if you consolidate cells)\n",
    "custom_nltk_data_path = r'C:\\Users\\ujjwa\\OneDrive\\Desktop\\Python\\nltk_data'\n",
    "if custom_nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(custom_nltk_data_path)\n",
    "\n",
    "print(\"NLTK data paths after explicit addition (if not present):\")\n",
    "for path in nltk.data.path:\n",
    "    print(path)\n",
    "\n",
    "\n",
    "# Ensure NLTK data is downloaded.\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading 'stopwords' corpus...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading 'punkt' tokenizer...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    print(\"Downloading 'averaged_perceptron_tagger'...\")\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"\\nNLTK data is ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
